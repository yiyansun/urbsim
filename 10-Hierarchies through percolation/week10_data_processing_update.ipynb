{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beab0ecf",
   "metadata": {},
   "source": [
    "### CASA0002\n",
    "\n",
    "# Urban Simulation\n",
    "***\n",
    "## Working with real world data\n",
    "\n",
    "Mateo Neira\n",
    "***\n",
    "\n",
    "Working with real world data comes with its own set of challenges. In this practical we will go over downloading and processing the public TFL multi-rail demand data set for London, which can be found [here](http://crowding.data.tfl.gov.uk).\n",
    "\n",
    "We will be using the data from the [NUMBAT project](http://crowding.data.tfl.gov.uk/NUMBAT/Intro_to_NUMBAT.pdf) -A comprehensive multi-rail demand data set for London. the NUMBAT dataset provides statistics on usage and travel patterns on TfL railway services. It is the most comprehensive multi-rail demand data set available for London."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2781dd",
   "metadata": {},
   "source": [
    "### TFL Origin Destination data\n",
    "***\n",
    "\n",
    "We will be using the 2019 data set (as the 2020 dataset coincides with lockdown measures). The data TFL provides represents the travel demand on a typical autumn weekday (Monday-Thursday), Friday, Saturday, and Sunday at all stations and lines of the London Underground, London Overground, Docklands Light Railway, TfL Rail / Elizabeth Line (crossrail) and London Trams.\n",
    "\n",
    "---\n",
    "### Map of the network we will be constructing\n",
    "\n",
    "![img](https://tfl.gov.uk/cdn/static/cms/images/london-rail-and-tube-services-map.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd381822",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "We need to perform the folling steps to get the data we need in the right format for to run the analysis required for the final assesment:\n",
    "\n",
    "\n",
    "1. Download raw data\n",
    "    * Get flow data from TFL \n",
    "    * Get geometries of the london rail & tube network from OSM\n",
    "2. Preprocess the data\n",
    "    * Data wrangling of flow data from TFL\n",
    "    * Transform geometry of london rail & tube network into a graph\n",
    "3. Merge TFL data and london underground shapefile\n",
    "    * Calculate flow data to input as weights in london network graph\n",
    "    * Calculate distances between stations to create file to be used in the spatial interaction model\n",
    "\n",
    "---\n",
    "_The raw data has already been provided for you in data /data folder. You can still explore the data directly from the links provided. In the case of the London rail & tube network geometries, these have been extracted from OSM._ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11466dd8",
   "metadata": {},
   "source": [
    "### Data Wrangling of passenger flows\n",
    "\n",
    "We will be focusing only on the data that covers Monday-Thursday. \n",
    "\n",
    "The first file we need is the [NBT19_Definitions.xlsx](http://crowding.data.tfl.gov.uk/NUMBAT/NUMBAT%202019/NBT19_Definitions.xlsx) file that contains important information on what  different codes mean. \n",
    "\n",
    "The rest of the files relate to the OD matrix for the different lines organized by timebands which can be found here: \n",
    "\n",
    "* [NBT19MTT2b_od_DLR_tb_wf.csv](http://crowding.data.tfl.gov.uk/NUMBAT/NUMBAT%202019/NBT19_OD_data/NBT19MTT2b_od__DLR_tb_wf.csv)\n",
    "* [NBT19MTT2b_od__EZL_tb_wf.csv](http://crowding.data.tfl.gov.uk/NUMBAT/NUMBAT%202019/NBT19_OD_data/NBT19MTT2b_od__EZL_tb_wf.csv)\n",
    "* [NBT19MTT2b_od__LO_tb_wf.csv](http://crowding.data.tfl.gov.uk/NUMBAT/NUMBAT%202019/NBT19_OD_data/NBT19MTT2b_od__LO_tb_wf.csv)\n",
    "* [NBT19MTT2b_od__LU_tb_wf.csv](http://crowding.data.tfl.gov.uk/NUMBAT/NUMBAT%202019/NBT19_OD_data/NBT19MTT2b_od__LU_tb_wf.csv)\n",
    "\n",
    "_*note that the MTT code in the file name refers to Monday to Thursday data as specified in the definitions file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will import all the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import json\n",
    "import re\n",
    "from shapely.geometry import Point, LineString #this library is for manipulating geometric objects, and it is what geopandas uses to store geometries\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tlf provides passenger flow data for each network separately\n",
    "#we need to create a list of all the different files we need\n",
    "files = ['NBT19MTT2b_od__DLR_tb_wf.csv', \n",
    "         'NBT19MTT2b_od__EZL_tb_wf.csv', \n",
    "         'NBT19MTT2b_od__LO_tb_wf.csv', \n",
    "         'NBT19MTT2b_od__LU_tb_wf.csv']\n",
    "\n",
    "#create an empty list to store dataframes for each file\n",
    "dfs = []\n",
    "\n",
    "#iterate through our files list, read the file and append to our dataframe list\n",
    "for file in files:\n",
    "    dfs.append(pd.read_csv('data/' + file))\n",
    "    \n",
    "#merge all dataframes\n",
    "london_OD = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f44ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#take a look at our data\n",
    "london_OD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb900e",
   "metadata": {},
   "source": [
    "#### Understanding our data\n",
    "\n",
    "If we read the information on the **Numbat Project** which as linked previously we can see that the data covers 15-minute periods. The specific datasets we are using are the O/D by network which registers number of trips between Origin and Destination station by time band.\n",
    "\n",
    "---\n",
    "* mode_mnlc_o: code of origin station\n",
    "* mode_mnlc_d: code of destionation station\n",
    "* \\[1,2,...,7,8\\]: number of passengers in respective timeband\n",
    "* mode: type of network (London Undeground, London Overgroud, DRL, ...)\n",
    "\n",
    "\n",
    "Since we are only interested in home-to-work trips, we will only keep the data pertaining to the morning peak (7am - 10am). Again, if we look at the definition file we can see that this is timeband 3.\n",
    "\n",
    "Additionally, we need to map the station codes to station names, we will be using the definition file for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b4be3",
   "metadata": {},
   "source": [
    "### Get the mnlc codes \n",
    "\n",
    "We will use the **NBT19_Definitions.xlsx** file to get the codes to match station names.\n",
    "\n",
    "```python\n",
    "pd.read_excel()\n",
    "```\n",
    "\n",
    "The function requires the **openpyxl** library to work. this can be installed using pip or conda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_df = pd.read_excel('data/NBT19_Definitions.xlsx', 'Stations')\n",
    "definition_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will drop any missing values from our dataframe\n",
    "definition_df.dropna(inplace=True)\n",
    "\n",
    "#transform mnlc codes from floats to int\n",
    "definition_df.MNLC = definition_df.MNLC.astype(int)\n",
    "definition_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e20f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the station names\n",
    "london_OD['station_origin'] = london_OD['mode_mnlc_o'].apply(lambda x: definition_df[definition_df.MNLC==x]['StationName'].values[0])\n",
    "london_OD['station_destination'] = london_OD['mode_mnlc_d'].apply(lambda x: definition_df[definition_df.MNLC==x]['StationName'].values[0])\n",
    "london_OD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20492e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will only keep passenger flows for the timeband we are interested in\n",
    "london_OD_AMpeak = london_OD[['station_origin', 'station_destination', '3']].copy()\n",
    "london_OD_AMpeak.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column\n",
    "london_OD_AMpeak.rename({'3':'flows'}, axis=1, inplace=True)\n",
    "\n",
    "#since the flows are averages the are stored as flows.\n",
    "#for our analysis we will turn the into ints\n",
    "london_OD_AMpeak.flows = london_OD_AMpeak.flows.astype(int)\n",
    "london_OD_AMpeak.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b587f",
   "metadata": {},
   "source": [
    "Let's get all the station names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_origin = list(london_OD_AMpeak.station_origin.values)\n",
    "station_destination = list(london_OD_AMpeak.station_destination.values)\n",
    "all_stations = list(set(station_origin + station_destination))\n",
    "all_stations = sorted(all_stations, key=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85652f58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b85be",
   "metadata": {},
   "source": [
    "We will need to clean this, as we want all stations that are the same summed. for example Hammersmith (DIS) and Hammersmith (H&C) are the same station. so we want these flows to be summed into one one just called Hammersmith. For this we will simplify all names by:\n",
    "* removing all text in between ()\n",
    "* removing LU, LO, NR, Tfl,  DLR\n",
    "\n",
    "We will use [RegEx](https://en.wikipedia.org/wiki/Regular_expression) to match and replace station names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e0edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_OD_AMpeak['station_origin']= london_OD_AMpeak.station_origin.apply(lambda x: \n",
    "                                      re.sub('\\sLU\\s?|\\sLO\\s?|\\sNR\\s?|\\sTf[lL]\\s?|\\sDLR\\s?|\\s\\(.*\\)', '', x)\n",
    "                                     )\n",
    "london_OD_AMpeak['station_destination']=london_OD_AMpeak.station_destination.apply(lambda x: \n",
    "                                      re.sub('\\sLU\\s?|\\sLO\\s?|\\sNR\\s?|\\sTf[lL]\\s?|\\sDLR\\s?|\\s\\(.*\\)', '', x)\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb687d3e",
   "metadata": {},
   "source": [
    "now we will sum the flows of stations with the same origin-destination name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ea216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by origin and destination station and sum the flows\n",
    "london_OD_AMpeak = london_OD_AMpeak.groupby(['station_origin', 'station_destination'], as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae5a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_OD_AMpeak.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ddad9",
   "metadata": {},
   "source": [
    "#### Does our data make sense?\n",
    "\n",
    "* Do the stations with least and most in and out flows make sense?\n",
    "* What type of data distribtion would we expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b026aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's sanity check and see the highest and lowest flows in and out flows\n",
    "outflows = london_OD_AMpeak.groupby('station_origin', as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a75704",
   "metadata": {},
   "outputs": [],
   "source": [
    "outflows.sort_values('flows').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14434ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "outflows.sort_values('flows').tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edaeeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's sanity check and see the highest and lowest flows in and out flows\n",
    "inflows = london_OD_AMpeak.groupby('station_destination', as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "inflows.sort_values('flows').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "inflows.sort_values('flows').tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1540db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot this as a histogram\n",
    "bin_size=100\n",
    "fig, ax = plt.subplots(2,1,figsize=(12,10))\n",
    "ax[0].hist(outflows['flows'], bins=int(outflows['flows'].max()/bin_size))\n",
    "ax[0].set_xlabel('London am peak outflows', fontsize = 15)\n",
    "ax[0].set_ylabel(\"Count\", fontsize= 15)\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].hist(inflows['flows'], bins=int(inflows['flows'].max()/bin_size))\n",
    "ax[1].set_xlabel('London am peak inflows', fontsize = 15)\n",
    "ax[1].set_ylabel(\"Count\", fontsize= 15)\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3de4d",
   "metadata": {},
   "source": [
    "### Load London Underground shapefile data\n",
    "\n",
    "Now that we have our Origin-Destination data, we need to construct a network representing the london rail & tube services. This is a bit more tricky, because we need both a topologically correct network, plus clean geometries so we can calculate distances to use as weights. \n",
    "\n",
    "The raw data we are using comes from OSM and it's inherently messy. We have two seperate files, one for stations and another for the lines.\n",
    "\n",
    "**These are the types of errors we will encounter:**\n",
    "\n",
    "![errors](img/errors.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data using geopandas\n",
    "stations = gpd.read_file('data/tfl_stations.json')\n",
    "lines = gpd.read_file('data/tfl_lines.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9311ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project data to British National Grid\n",
    "#we do this so we can work in meters\n",
    "stations = stations.to_crs(epsg = 27700)\n",
    "lines = lines.to_crs(epsg = 27700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553369d1",
   "metadata": {},
   "source": [
    "### Stations file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8f268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4c5a0",
   "metadata": {},
   "source": [
    "### Clean station names\n",
    "\n",
    "We have multiple points representing one station (for example Paddington has seperate points for different entrances). We will simplify this by using RegEx make the names consistent. \n",
    "\n",
    "Once we have cleaned the names we can set the coordinate of these stations to the mean values of the coordinates of all the stations with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daae6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all station names\n",
    "station_names = {row.id: row['name'] for i, row in stations.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean station names\n",
    "for k, v in station_names.items():\n",
    "    if k == 'none':\n",
    "        continue\n",
    "    cleaned_name = re.sub('\\sLU\\s?|\\sLO\\s?|\\sNR\\s?|\\sTf[lL]\\s?|\\sDLR\\s?|\\s\\(.*\\)', '', v)\n",
    "    \n",
    "    #note that bank and monument are enconded as one station in our TFL data, so we will do the same\n",
    "    if cleaned_name in ['Bank', 'Monument']:\n",
    "        cleaned_name = 'Bank and Monument'\n",
    "    station_names[k] = cleaned_name\n",
    "    \n",
    "stations['name'] = stations['id'].apply(lambda x: station_names[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will plot all the stations with multiple points to check there are no errors\n",
    "fig, ax = plt.subplots(figsize = (30,30))\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.axis('off')\n",
    "ax.margins(0.2)\n",
    "color = iter(cm.rainbow(np.linspace(0, 1, 29)))\n",
    "stations.plot(ax=ax, color='grey', markersize=0.5)\n",
    "for i, group in stations.groupby('name'):\n",
    "    if len(group)>1:\n",
    "        group.plot(ax=ax, color = next(color), markersize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d203df",
   "metadata": {},
   "source": [
    "**Change position of station to mean of all station with the same name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb3ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get position of stations as mean of x, y of stations\n",
    "\n",
    "#create empty dict to store list of coordinates\n",
    "pos = {}\n",
    "\n",
    "#iterate through the stations\n",
    "for i, station in stations.iterrows():\n",
    "    xy = station.geometry.coords[0]\n",
    "    \n",
    "    #if station already in dict add coordinate to coordinate list\n",
    "    if station['name'] in pos.keys():\n",
    "        pos[station['name']].extend([xy])\n",
    "    #if station is not in the dict add station to dict and set first coordinate in list\n",
    "    else:\n",
    "        pos[station['name']] = [xy]\n",
    "\n",
    "#iterate through our dict and replace coordinate list with mean value\n",
    "for k, v in pos.items():\n",
    "    #we set axis to 0 to make sure to take mean of x and y coordinates\n",
    "    pos[k] = np.mean(v, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set new geometry\n",
    "stations['geometry'] = stations['name'].apply(lambda x: Point(pos[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check plot to make sure there are no errors\n",
    "stations.plot(markersize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346ea38",
   "metadata": {},
   "source": [
    "### Lines file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5222dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e407d47c",
   "metadata": {},
   "source": [
    "### Get all line names\n",
    "\n",
    "We can see from our lines geodataframe that one geometry can represent multiple lines (for example one for the circle line and another for hammersmith and city) that go between the same stations. \n",
    "\n",
    "Let's work each line seperately to make cleaning and checking the data easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f654037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all line types as a list and subset geoms by lines\n",
    "line_names  = lines.lines.apply(lambda x: [x['name'] for x in json.loads(x)] )\n",
    "line_names = list(set([item for sublist in line_names for item in sublist]))\n",
    "line_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7822b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data for each line\n",
    "for line in line_names:\n",
    "    #I'm removing thameslink 6tph line because there are not corresponding stations in station file\n",
    "    if line == 'Thameslink 6tph line':\n",
    "        continue\n",
    "    fig, ax = plt.subplots(figsize = (7,7))\n",
    "    fig.suptitle(f'line: {line}')\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis('off')\n",
    "    ax.margins(0.1)\n",
    "    temp_lines = lines[lines.lines.str.contains(line)]\n",
    "    base = temp_lines.plot(ax=ax)\n",
    "    temp_stations = stations[stations.lines.str.contains(line)]\n",
    "    temp_stations.plot(ax=base, markersize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dccbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see that east london line is incomplete\n",
    "#digging into this however we notice that it's because east london is actually part of the overground\n",
    "#merge East London line and London Overground since they are one line\n",
    "lines.lines = lines.lines.str.replace('East London', 'London Overground')\n",
    "stations.lines = stations.lines.str.replace('East London', 'London Overground')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9bd826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our TFL data doesn't include tramlink, emirates air lines, and crossrail is still in construction (except not really)\n",
    "#exclude the lines that we are not going to use\n",
    "excluded_lines = ['Thameslink 6tph line', 'East London', 'Crossrail 2', 'Emirates Air Line', 'Crossrail', 'Tramlink']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e925bb",
   "metadata": {},
   "source": [
    "## Create seperate graphs for each line and then join\n",
    "\n",
    "Since we don't have properly constructed line geometries because:\n",
    "* there are discontinuities in the line geometries\n",
    "* a line between two stations can actually be composed of more than one geometry\n",
    "\n",
    "We will construct geometric graphs (meaning graphs just from the line geometries where nodes are coordinates that define the line geometry). It's easy to fix the discontinuities in this manner because the can be thought of as nodes with degree = 1 that have another node with degree = 1 within a threshold distance. \n",
    "\n",
    "Once we have these discontinuties fixed we can use these geometric graphs to construct proper geometries between stations by taking the shortest paths within these networks starting from the node closest to the origin station and ending in the node closest to the destination station. The shortest path will then be a list of coordinates that define the line geometry between the two stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty dict to hold our geometric graphs\n",
    "geometric_graphs = {}\n",
    "\n",
    "#iterate through each line individually\n",
    "for line_name in line_names:\n",
    "    #skip lines that we won't be using\n",
    "    if line_name in excluded_lines :\n",
    "        continue\n",
    "    print(line_name)\n",
    "    \n",
    "    #subset our line and station files for the ones the belong to a specific line\n",
    "    temp_lines = lines[lines.lines.str.contains(line_name)]\n",
    "    temp_stations = stations[stations.lines.str.contains(line_name)]\n",
    "    \n",
    "    #create empty graph\n",
    "    G= nx.Graph()\n",
    "    \n",
    "    #iterate through our line geometries\n",
    "    for i, line in temp_lines.iterrows():\n",
    "        #get list of coordinates that define our line\n",
    "        _l = list(line.geometry.coords)\n",
    "\n",
    "        #add coordinates as edges\n",
    "        G.add_edges_from(list(zip(_l,_l[1:])))\n",
    "    \n",
    "    #remove any self loops\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    \n",
    "    #the position of the node is the name of the node\n",
    "    #let's extract this to plot our graph\n",
    "    pos = {n: n for n in G.nodes()}\n",
    "    \n",
    "    # get degree one nodes\n",
    "    D = nx.degree(G)\n",
    "    k1 = [node for node,degree in D if degree==1]\n",
    "    #let's plot our graph and all degree one nodes\n",
    "    nx.draw(G, pos=pos, node_size=0.5, node_color='grey')\n",
    "    nx.draw_networkx_nodes(G, pos=pos, nodelist=k1, node_color='red', node_size=8)\n",
    "    plt.show()\n",
    "    \n",
    "    #get distance between all points of 1-degree, this will be a matrix\n",
    "    dist = distance.cdist(k1, k1)\n",
    "    \n",
    "    #add edges between points that are close\n",
    "    for j in range(dist.shape[0]-1):\n",
    "        temp = dist[j][j+1:]\n",
    "        #get index of closest node\n",
    "        i_min = np.argmin(temp)\n",
    "        if dist[j][i_min+j+1]<50:\n",
    "            G.add_edge(k1[j],k1[i_min+j+1])\n",
    "    \n",
    "    #recalculate degree\n",
    "    D = nx.degree(G)\n",
    "    k1 = [node for node,degree in D if degree==1]\n",
    "    #plot new graph\n",
    "    nx.draw(G, pos=pos, node_size=0.5, node_color='grey')\n",
    "    nx.draw_networkx_nodes(G, pos=pos, nodelist=k1, node_color='red', node_size=8)\n",
    "    plt.show()\n",
    "    print(nx.number_connected_components(G))\n",
    "    #store graph in dictionary\n",
    "    geometric_graphs[line_name] = G\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3400d606",
   "metadata": {},
   "source": [
    "Now that we have these geometric graphs we can create our final graph by using the data in the lines attribute in our lines geodataframe that contains origin and destination station. We can then use our geometric graph and find the shortest route to create our line geometries and find the length of the lines. \n",
    "\n",
    "We will store this information in a dataframe that we can use to create our final graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6400cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with all geometries from all valid lines\n",
    "#this list will contain a dictionary for each edge in our graph that contains\n",
    "# node pairs, line name, and a geometry (this is just for quick visualization purposes and will be replaced by true geometry)\n",
    "edge_list = []\n",
    "def _has_ids(k):\n",
    "    #helper function to check if line contains id of start and end station\n",
    "    if ('start_sid' in k.keys()) and ('end_sid' in k.keys()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#iterate through each line seperately \n",
    "for line_name in line_names:\n",
    "    if line_name in excluded_lines :\n",
    "        continue\n",
    "        \n",
    "    #subset lines and stations\n",
    "    temp_lines = lines[lines.lines.str.contains(line_name)]\n",
    "    temp_stations = stations[stations.lines.str.contains(line_name)]\n",
    "    \n",
    "    #for each line in our subset dataframe get each origin and destination pair\n",
    "    for i, line in temp_lines.iterrows():\n",
    "        for k in json.loads(line.lines):\n",
    "            if k['name'] == line_name:\n",
    "                if _has_ids(k):\n",
    "                    #get start and end station ids\n",
    "                    start_id = k['start_sid']\n",
    "                    end_id = k['end_sid']\n",
    "\n",
    "                    #get info of start and end station\n",
    "                    start_station = temp_stations[(temp_stations.id == start_id) | (temp_stations.altmodeid == start_id)].values\n",
    "                    end_station = temp_stations[(temp_stations.id == end_id) | (temp_stations.altmodeid == end_id)].values\n",
    "                    \n",
    "                    #if no matching station exist, let's just take the starting and ending coordinate of the line for now\n",
    "                    if len(start_station) >= 1 and len(end_station) >=1:\n",
    "                        s_geom = start_station[0][-1].coords[0]\n",
    "                        e_geom = end_station[0][-1].coords[0]\n",
    "                        edge_list.append({\n",
    "                            'line_name': line_name,\n",
    "                            'start_id': start_id,\n",
    "                            'end_id': end_id,\n",
    "                            'geometry': LineString([s_geom, e_geom])\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn our dictionary list into a geopandas df to quickly plot and sanity check our work\n",
    "edge_gdf = gpd.GeoDataFrame(edge_list)\n",
    "edge_gdf.crs = lines.crs\n",
    "edge_gdf.plot()\n",
    "edge_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add geometry information back into the graph\n",
    "for line_name in line_names:\n",
    "    if line_name in excluded_lines :\n",
    "        continue\n",
    "    temp_lines = edge_gdf[edge_gdf.line_name == line_name]\n",
    "    temp_stations = stations[stations.lines.str.contains(line_name)]\n",
    "    \n",
    "    #let's plot our lines so we can check them\n",
    "    fig, ax = plt.subplots(figsize = (7,7))\n",
    "    fig.suptitle(f'line: {line_name}')\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis('off')\n",
    "    ax.margins(0.1)\n",
    "    \n",
    "    base = temp_lines.plot(ax=ax, color='grey')\n",
    "    temp_stations.plot(ax=base, markersize=20, color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    #add real geometry back\n",
    "    for i, edge in temp_lines.iterrows():\n",
    "        #get start and end of line coordinates\n",
    "        start = edge.geometry.coords[0]\n",
    "        end = edge.geometry.coords[-1]\n",
    "        \n",
    "        #let's get all the coordinates in our geometric graph\n",
    "        nodes = list(geometric_graphs[line_name].nodes)\n",
    "        \n",
    "        #find nearest node in graph to origin and destination\n",
    "        s_dist = distance.cdist([start], nodes)[0]\n",
    "        s_i = np.argmin(s_dist)\n",
    "        source= nodes[s_i]\n",
    "        \n",
    "        t_dist = distance.cdist([end], nodes)[0]\n",
    "        t_i = np.argmin(t_dist)\n",
    "        target= nodes[t_i]\n",
    "        \n",
    "        #get shortest path\n",
    "        sp = nx.shortest_path(geometric_graphs[line_name], source, target)\n",
    "        \n",
    "        #make into geometry\n",
    "        #notice how I'm also adding the start and end coordinates to the line definition\n",
    "        #this fixes the problem of stations and lines not matching up\n",
    "        geometry = LineString([start] + sp + [end])\n",
    "        edge_gdf.loc[i, 'geometry'] = geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot to sanity check\n",
    "edge_gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4369d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's store the length of the real geometry\n",
    "edge_gdf['length'] = edge_gdf.geometry.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c774ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is our resulting dataframe\n",
    "edge_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8215ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's replace the ids with the actual name\n",
    "#get all station names\n",
    "station_names_2 = {row.altmodeid: row['name'] for i, row in stations.iterrows() if row.altmodeid != None}\n",
    "station_names.update(station_names_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd81c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_gdf['start_id'] = edge_gdf['start_id'].apply(lambda x: station_names[x])\n",
    "edge_gdf['end_id'] = edge_gdf['end_id'].apply(lambda x: station_names[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e952da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can use this to create our network!\n",
    "G = nx.from_pandas_edgelist(edge_gdf, source = 'start_id', target='end_id', edge_attr=['length', 'line_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot\n",
    "#notice that even though we created the real geometries, the graph still plots just straight line\n",
    "#this is because networkx doesn't have a concept of a edge geometry\n",
    "#however this is ok, since we only need the distance as weights, and that is already an attribute in our graph\n",
    "pos = {row['name']: row.geometry.coords[0] for i, row in stations.iterrows() if row['name'] in G.nodes()}\n",
    "Gcc = nx.connected_components(G)\n",
    "for n in Gcc:\n",
    "    G_sub = G.subgraph(n)\n",
    "    lines = [data['line_name'] for u,v, data in G_sub.edges(data=True)]\n",
    "    print(set(lines))\n",
    "    nx.draw(G_sub, pos, node_size=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8693025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save position to graph\n",
    "nx.set_node_attributes(G, pos, 'coords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85a751",
   "metadata": {},
   "source": [
    "### Sanity check our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that all distances are greater or equal to euclidean distance\n",
    "for u,v, data in G.edges(data=True):\n",
    "    assert(data['length'] >= distance.euclidean(pos[u], pos[v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5706ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if want to know what is the maximum distance between stations\n",
    "max(dict(G.edges).items(), key=lambda x: x[1]['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if want to know to which stations Baker Street is directly connected to\n",
    "Baker_Street = [(u,v) for  u,v in G.edges() if u == 'Baker Street' or v == 'Baker Street']\n",
    "Baker_Street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also check the degree of the nodes in our network and check that they make sense\n",
    "deg_london = nx.degree(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index = dict(deg_london).keys())\n",
    "df['degree'] = dict(deg_london).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('degree', ascending =False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421a4d5",
   "metadata": {},
   "source": [
    "### Merge Network with OD data\n",
    "\n",
    "ok, we are almost done!\n",
    "\n",
    "We have processed our raw data, now we need to combine the two (TLF OD and our Network). For this we need two types of merge:\n",
    "\n",
    "1. add flows as weights to the network (to be able to calculate disruptions to the network)\n",
    "2. create OD with distance for our spatial interaction models (for our spatial interaction models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0569a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that our OD data and network data match\n",
    "OD_names = set(london_OD_AMpeak.station_origin.unique())\n",
    "network_names = set([n for n in G.nodes()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ad5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_names.symmetric_difference(OD_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e64b5a",
   "metadata": {},
   "source": [
    "Ok, we have a few stations that don't match up. They are small enough that we can solve this quickly by creating a mapping between names of the stations that we can keep.\n",
    "\n",
    "* Battersea power station and Nine Elms data doesn't exist in our TFL data because the station wasn't opened when the data was collected. This was an extension of Northern line that opened recently.\n",
    "* Action Main Line, Hanwell, Hayes & Harlington, Southall, and West Ealing are part of crossrail which we are not considering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ba291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_map = {\n",
    "    'Heathrow Terminal 4 EL': 'Heathrow Terminal 4',\n",
    "    'Heathrow Terminals 123': 'Heathrow Terminals 2 & 3',\n",
    "    'Heathrow Terminals 2 & 3 EL': 'Heathrow Terminals 2 & 3',\n",
    "    \"Walthamstow Queen's Road\": 'Walthamstow Queens Road'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb46451",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_OD_AMpeak.station_origin = london_OD_AMpeak.station_origin.apply(\n",
    "    lambda x: name_map[x] if x in name_map.keys() else x\n",
    ")\n",
    "london_OD_AMpeak.station_destination = london_OD_AMpeak.station_destination.apply(\n",
    "    lambda x: name_map[x] if x in name_map.keys() else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's filter out the stations that we don't have in our network\n",
    "OD_names = set(london_OD_AMpeak.station_origin.unique())\n",
    "_filter = list(network_names.symmetric_difference(OD_names))\n",
    "_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ead39",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_OD_AMpeak = london_OD_AMpeak[~london_OD_AMpeak.station_origin.isin(_filter)]\n",
    "london_OD_AMpeak = london_OD_AMpeak[~london_OD_AMpeak.station_destination.isin(_filter)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cdf77",
   "metadata": {},
   "source": [
    "**Add flow data to our network**\n",
    "\n",
    "Our TFL data contains flows for OD pairs, but we don't know the flows passing through each edge in our network. We will have to calculate this assuming fall people travelling from Origin to Destination station are taking the shortest path within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c23d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary to store flows for all edges\n",
    "flows = {(u,v): 0 for u,v in G.edges()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e334776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate shortest paths for all flows and add data to dict\n",
    "for i, row in london_OD_AMpeak.iterrows():\n",
    "    source = row.station_origin\n",
    "    target = row.station_destination\n",
    "    \n",
    "    #get shortest path\n",
    "    path = nx.dijkstra_path(G, source, target)\n",
    "    \n",
    "    #our path is a list of nodes, we need to turn this to a list of edges\n",
    "    path_edges = list(zip(path,path[1:])) \n",
    "    \n",
    "    #add flows to our dict\n",
    "    for u,v in path_edges:\n",
    "        try:\n",
    "            flows[(u,v)] += row.flows\n",
    "        except:\n",
    "            flows[(v,u)] += row.flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c62382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this as a network attribute\n",
    "nx.set_edge_attributes(G, flows, 'flows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccca12e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot our calcuated flows\n",
    "flows_values = flows.values()\n",
    "flow_color=[(i[2]['flows']/max(flows_values)) for i in G.edges(data=True)]\n",
    "flow_width=[(i[2]['flows']/max(flows_values)*10) for i in G.edges(data=True)]\n",
    "\n",
    "# Plot graph\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "#pos=nx.spring_layout(X)\n",
    "edg=nx.draw_networkx_edges(G, pos,edge_color=flow_color, width=flow_width)\n",
    "\n",
    "nx.draw_networkx_nodes(G,\n",
    "        pos = pos,\n",
    "        node_color= 'black',\n",
    "        node_size= 1)\n",
    "\n",
    "plt.colorbar(edg,label=\"Passenger Flows\",orientation=\"horizontal\", shrink=0.5)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"London network Passenger Flows\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get edge with highest number of flows\n",
    "max(flows, key=flows.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8198278",
   "metadata": {},
   "source": [
    "What is going on here? Should we fix it or should we just consider it as a limitation of our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get edge data with lowest number of flows\n",
    "min(flows, key=flows.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88100e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save graph\n",
    "for i in G.nodes():\n",
    "    G.nodes[i]['coords'] = str(G.nodes[i]['coords'])\n",
    "nx.write_graphml_lxml(G, \"outputs/london_updated.graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee394a2",
   "metadata": {},
   "source": [
    "**Create OD pairs with distance, population and jobs**\n",
    "\n",
    "Lastly for our spatial interaction we need to add distance data to our OD list, as well as population and jobs.\n",
    "Since we don't have population and jobs data at a station level, we will use total in and out flows as proxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create Origin Destination matrix\n",
    "OD = pd.pivot_table(london_OD_AMpeak, \n",
    "                    values =\"flows\", \n",
    "                    index=\"station_origin\", \n",
    "                    columns = \"station_destination\",\n",
    "                    aggfunc=sum, \n",
    "                    margins=True)\n",
    "OD.fillna(0, inplace=True)\n",
    "OD = OD.astype(int)\n",
    "OD.to_csv('outputs/OD_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf48af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OD.sort_values(by='All').tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e1e96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#lets get total at origin and total at destinations\n",
    "origin = OD.loc[:,'All'].to_dict()\n",
    "destination = OD.loc['All',:].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets add this data as population and jobs\n",
    "london_OD_AMpeak['population'] = london_OD_AMpeak.station_origin.apply(lambda x: origin[x])\n",
    "london_OD_AMpeak['jobs'] = london_OD_AMpeak.station_destination.apply(lambda x: destination[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d1365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get distance between stations withion the network\n",
    "london_OD_AMpeak['distance'] = -1\n",
    "for i, row in london_OD_AMpeak.iterrows():\n",
    "    source = row.station_origin\n",
    "    target = row.station_destination\n",
    "    distance = nx.shortest_path_length(G, source, target, weight='length', method='dijkstra')\n",
    "    london_OD_AMpeak.loc[i, 'distance'] = distance\n",
    "\n",
    "# or you could use all_pairs_dijkstra_path_length(G, weight='length') to get all distances at once..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8919aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_OD_AMpeak.to_csv('outputs/london_flows.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fee339",
   "metadata": {},
   "source": [
    "There was some confusion about missing potential flows in the network for the assignment. This is how you would do the complete square matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfacef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the ALL column and row\n",
    "OD_melt = OD.iloc[:-1,:-1]\n",
    "# melt OD back to a long dataframe\n",
    "OD_melt = OD_melt.reset_index().melt(id_vars='station_origin', value_vars=OD.columns[:-1])\n",
    "OD_melt.columns = ['station_origin', 'station_destination', 'flows']\n",
    "# remove the self flows\n",
    "OD_melt = OD_melt[OD_melt.station_origin != OD_melt.station_destination]\n",
    "OD_melt.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ae62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_melt['population'] = OD_melt.station_origin.apply(lambda x: origin[x])\n",
    "OD_melt['jobs'] = OD_melt.station_destination.apply(lambda x: destination[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fdf0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distance between stations withion the network\n",
    "OD_melt['distance'] = -1\n",
    "for i, row in OD_melt.iterrows():\n",
    "    source = row.station_origin\n",
    "    target = row.station_destination\n",
    "    distance = nx.shortest_path_length(G, source, target, weight='length', method='dijkstra')\n",
    "    OD_melt.loc[i, 'distance'] = distance\n",
    "\n",
    "print(OD_melt.shape)\n",
    "OD_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_melt.sort_values('flows', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot flows against distance, both in the log scale\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(OD_melt.distance, OD_melt.flows, alpha=0.5)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Distance (m)')\n",
    "ax.set_ylabel('Flows')\n",
    "ax.set_title('Flows vs Distance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8fa52b8",
   "metadata": {},
   "source": [
    "# exercise\n",
    "#### Part A \n",
    "Let's calculate the flows between 2 stations in the network by combing origin-destinction data with topogical data.\n",
    "\n",
    "+ Get the flows between 2 stations that are separated by a single station (a --> b --> c) on the network\n",
    "+ Find a way to separate out the flows between a and c from those between a and b and b and c\n",
    "\n",
    "#### Part B\n",
    "What are the limitations of this undirected network? Let's go back and think about a directed network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate our network but with directional Graph by specifying create_using parameter!\n",
    "G_di = nx.from_pandas_edgelist(edge_gdf, source = 'start_id', target='end_id', edge_attr=['length', 'line_name'], create_using=nx.DiGraph)\n",
    "\n",
    "# we need to add an extra step to iterate through the edges from the opposite direction \n",
    "for i, edge in edge_gdf.iterrows():\n",
    "    if not G_di.has_edge(edge['end_id'], edge['start_id']):\n",
    "        G_di.add_edge(edge['end_id'], edge['start_id'], length=edge['length'], line_name=edge['line_name'])\n",
    "\n",
    "# adding the position of the nodes\n",
    "pos = {row['name']: row.geometry.coords[0] for i, row in stations.iterrows() if row['name'] in G_di.nodes()}\n",
    "nx.set_node_attributes(G_di, pos, 'coords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e50fe",
   "metadata": {},
   "source": [
    "What are some questions that you can ask with this directed network that you can't with the undirected network?\n",
    "\n",
    "+ Let's call a station's \"directionality\" the difference between the number of people going through a station in one direction and the other.\n",
    "+ Which stations have the highest directionality?\n",
    "+ What about the lowest? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "locomizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
